{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0e6b9614",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import category_encoders as ce\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "\n",
    "\n",
    "# \n",
    "def load_data(file_path):\n",
    "    \"\"\"Load data from a CSV file into a pandas DataFrame.\"\"\"\n",
    "    return pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "76c0d10b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(355190, 116)\n"
     ]
    }
   ],
   "source": [
    "df = load_data('data/bank_data_train.csv')\n",
    "print(df.shape)\n",
    "\n",
    "# Define a threshold for missing values\n",
    "TRESHOLD = 0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1a8ee35c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 355190 entries, 0 to 355189\n",
      "Columns: 116 entries, ID to TARGET\n",
      "dtypes: float64(94), int64(9), object(13)\n",
      "memory usage: 314.3+ MB\n",
      "\n",
      "Dropped column: 'ID'\n",
      "\n",
      "DataFrame Description (numeric):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CR_PROD_CNT_IL</th>\n",
       "      <th>AMOUNT_RUB_CLO_PRC</th>\n",
       "      <th>PRC_ACCEPTS_A_EMAIL_LINK</th>\n",
       "      <th>APP_REGISTR_RGN_CODE</th>\n",
       "      <th>PRC_ACCEPTS_A_POS</th>\n",
       "      <th>PRC_ACCEPTS_A_TK</th>\n",
       "      <th>TURNOVER_DYNAMIC_IL_1M</th>\n",
       "      <th>CNT_TRAN_AUT_TENDENCY1M</th>\n",
       "      <th>SUM_TRAN_AUT_TENDENCY1M</th>\n",
       "      <th>AMOUNT_RUB_SUP_PRC</th>\n",
       "      <th>...</th>\n",
       "      <th>REST_DYNAMIC_CC_3M</th>\n",
       "      <th>MED_DEBT_PRC_YWZ</th>\n",
       "      <th>LDEAL_ACT_DAYS_PCT_TR3</th>\n",
       "      <th>LDEAL_ACT_DAYS_PCT_AAVG</th>\n",
       "      <th>LDEAL_DELINQ_PER_MAXYWZ</th>\n",
       "      <th>TURNOVER_DYNAMIC_CC_3M</th>\n",
       "      <th>LDEAL_ACT_DAYS_PCT_TR</th>\n",
       "      <th>LDEAL_ACT_DAYS_PCT_TR4</th>\n",
       "      <th>LDEAL_ACT_DAYS_PCT_CURR</th>\n",
       "      <th>TARGET</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>355190.000000</td>\n",
       "      <td>316867.000000</td>\n",
       "      <td>155163.0</td>\n",
       "      <td>60550.000000</td>\n",
       "      <td>155163.0</td>\n",
       "      <td>155163.0</td>\n",
       "      <td>355190.000000</td>\n",
       "      <td>77112.000000</td>\n",
       "      <td>77112.000000</td>\n",
       "      <td>316867.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>355190.000000</td>\n",
       "      <td>95713.000000</td>\n",
       "      <td>93448.000000</td>\n",
       "      <td>98175.000000</td>\n",
       "      <td>95713.000000</td>\n",
       "      <td>355190.000000</td>\n",
       "      <td>93448.000000</td>\n",
       "      <td>93448.000000</td>\n",
       "      <td>93448.000000</td>\n",
       "      <td>355190.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.105225</td>\n",
       "      <td>0.044045</td>\n",
       "      <td>0.0</td>\n",
       "      <td>50.947498</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001305</td>\n",
       "      <td>0.416896</td>\n",
       "      <td>0.414572</td>\n",
       "      <td>0.085249</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007309</td>\n",
       "      <td>0.055074</td>\n",
       "      <td>0.025707</td>\n",
       "      <td>0.049943</td>\n",
       "      <td>0.009252</td>\n",
       "      <td>0.004309</td>\n",
       "      <td>0.013938</td>\n",
       "      <td>0.013938</td>\n",
       "      <td>0.013938</td>\n",
       "      <td>0.081435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.431372</td>\n",
       "      <td>0.108449</td>\n",
       "      <td>0.0</td>\n",
       "      <td>21.777855</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.029118</td>\n",
       "      <td>0.316493</td>\n",
       "      <td>0.338612</td>\n",
       "      <td>0.142310</td>\n",
       "      <td>...</td>\n",
       "      <td>0.066681</td>\n",
       "      <td>0.215909</td>\n",
       "      <td>0.115732</td>\n",
       "      <td>0.185830</td>\n",
       "      <td>0.092789</td>\n",
       "      <td>0.059852</td>\n",
       "      <td>0.097099</td>\n",
       "      <td>0.097099</td>\n",
       "      <td>0.097099</td>\n",
       "      <td>0.273503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006944</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>33.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.139645</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>54.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.027117</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.036608</td>\n",
       "      <td>0.0</td>\n",
       "      <td>72.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.661195</td>\n",
       "      <td>0.110005</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>11.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>89.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 102 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       CR_PROD_CNT_IL  AMOUNT_RUB_CLO_PRC  PRC_ACCEPTS_A_EMAIL_LINK  \\\n",
       "count   355190.000000       316867.000000                  155163.0   \n",
       "mean         0.105225            0.044045                       0.0   \n",
       "std          0.431372            0.108449                       0.0   \n",
       "min          0.000000            0.000000                       0.0   \n",
       "25%          0.000000            0.000000                       0.0   \n",
       "50%          0.000000            0.000000                       0.0   \n",
       "75%          0.000000            0.036608                       0.0   \n",
       "max         11.000000            1.000000                       0.0   \n",
       "\n",
       "       APP_REGISTR_RGN_CODE  PRC_ACCEPTS_A_POS  PRC_ACCEPTS_A_TK  \\\n",
       "count          60550.000000           155163.0          155163.0   \n",
       "mean              50.947498                0.0               0.0   \n",
       "std               21.777855                0.0               0.0   \n",
       "min                0.000000                0.0               0.0   \n",
       "25%               33.000000                0.0               0.0   \n",
       "50%               54.000000                0.0               0.0   \n",
       "75%               72.000000                0.0               0.0   \n",
       "max               89.000000                0.0               0.0   \n",
       "\n",
       "       TURNOVER_DYNAMIC_IL_1M  CNT_TRAN_AUT_TENDENCY1M  \\\n",
       "count           355190.000000             77112.000000   \n",
       "mean                 0.001305                 0.416896   \n",
       "std                  0.029118                 0.316493   \n",
       "min                  0.000000                 0.006944   \n",
       "25%                  0.000000                 0.166667   \n",
       "50%                  0.000000                 0.300000   \n",
       "75%                  0.000000                 0.571429   \n",
       "max                  1.000000                 1.000000   \n",
       "\n",
       "       SUM_TRAN_AUT_TENDENCY1M  AMOUNT_RUB_SUP_PRC  ...  REST_DYNAMIC_CC_3M  \\\n",
       "count             77112.000000       316867.000000  ...       355190.000000   \n",
       "mean                  0.414572            0.085249  ...            0.007309   \n",
       "std                   0.338612            0.142310  ...            0.066681   \n",
       "min                   0.000000            0.000000  ...            0.000000   \n",
       "25%                   0.139645            0.000000  ...            0.000000   \n",
       "50%                   0.285714            0.027117  ...            0.000000   \n",
       "75%                   0.661195            0.110005  ...            0.000000   \n",
       "max                   1.000000            1.000000  ...            1.000000   \n",
       "\n",
       "       MED_DEBT_PRC_YWZ  LDEAL_ACT_DAYS_PCT_TR3  LDEAL_ACT_DAYS_PCT_AAVG  \\\n",
       "count      95713.000000            93448.000000             98175.000000   \n",
       "mean           0.055074                0.025707                 0.049943   \n",
       "std            0.215909                0.115732                 0.185830   \n",
       "min            0.000000                0.000000                 0.000000   \n",
       "25%            0.000000                0.000000                 0.000000   \n",
       "50%            0.000000                0.000000                 0.000000   \n",
       "75%            0.000000                0.000000                 0.000000   \n",
       "max            1.000000                1.000000                 1.000000   \n",
       "\n",
       "       LDEAL_DELINQ_PER_MAXYWZ  TURNOVER_DYNAMIC_CC_3M  LDEAL_ACT_DAYS_PCT_TR  \\\n",
       "count             95713.000000           355190.000000           93448.000000   \n",
       "mean                  0.009252                0.004309               0.013938   \n",
       "std                   0.092789                0.059852               0.097099   \n",
       "min                   0.000000                0.000000               0.000000   \n",
       "25%                   0.000000                0.000000               0.000000   \n",
       "50%                   0.000000                0.000000               0.000000   \n",
       "75%                   0.000000                0.000000               0.000000   \n",
       "max                   1.000000                1.000000               1.000000   \n",
       "\n",
       "       LDEAL_ACT_DAYS_PCT_TR4  LDEAL_ACT_DAYS_PCT_CURR         TARGET  \n",
       "count            93448.000000             93448.000000  355190.000000  \n",
       "mean                 0.013938                 0.013938       0.081435  \n",
       "std                  0.097099                 0.097099       0.273503  \n",
       "min                  0.000000                 0.000000       0.000000  \n",
       "25%                  0.000000                 0.000000       0.000000  \n",
       "50%                  0.000000                 0.000000       0.000000  \n",
       "75%                  0.000000                 0.000000       0.000000  \n",
       "max                  1.000000                 1.000000       1.000000  \n",
       "\n",
       "[8 rows x 102 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DataFrame Description (object):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CLNT_TRUST_RELATION</th>\n",
       "      <th>APP_MARITAL_STATUS</th>\n",
       "      <th>APP_KIND_OF_PROP_HABITATION</th>\n",
       "      <th>CLNT_JOB_POSITION_TYPE</th>\n",
       "      <th>CLNT_JOB_POSITION</th>\n",
       "      <th>APP_DRIVING_LICENSE</th>\n",
       "      <th>APP_EDUCATION</th>\n",
       "      <th>APP_TRAVEL_PASS</th>\n",
       "      <th>APP_CAR</th>\n",
       "      <th>APP_POSITION_TYPE</th>\n",
       "      <th>APP_EMP_TYPE</th>\n",
       "      <th>APP_COMP_TYPE</th>\n",
       "      <th>PACK</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>69421</td>\n",
       "      <td>68234</td>\n",
       "      <td>59361</td>\n",
       "      <td>44781</td>\n",
       "      <td>210811</td>\n",
       "      <td>57257</td>\n",
       "      <td>68104</td>\n",
       "      <td>57257</td>\n",
       "      <td>57256</td>\n",
       "      <td>60545</td>\n",
       "      <td>67362</td>\n",
       "      <td>67362</td>\n",
       "      <td>355190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>21</td>\n",
       "      <td>13</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>19588</td>\n",
       "      <td>2</td>\n",
       "      <td>17</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>FRIEND</td>\n",
       "      <td>M</td>\n",
       "      <td>SO</td>\n",
       "      <td>SPECIALIST</td>\n",
       "      <td>ДИРЕКТОР</td>\n",
       "      <td>N</td>\n",
       "      <td>H</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>SPECIALIST</td>\n",
       "      <td>PRIVATE</td>\n",
       "      <td>PRIVATE</td>\n",
       "      <td>102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>24896</td>\n",
       "      <td>30724</td>\n",
       "      <td>28056</td>\n",
       "      <td>25123</td>\n",
       "      <td>11200</td>\n",
       "      <td>36332</td>\n",
       "      <td>42459</td>\n",
       "      <td>52750</td>\n",
       "      <td>32843</td>\n",
       "      <td>36622</td>\n",
       "      <td>59087</td>\n",
       "      <td>59087</td>\n",
       "      <td>116986</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       CLNT_TRUST_RELATION APP_MARITAL_STATUS APP_KIND_OF_PROP_HABITATION  \\\n",
       "count                69421              68234                       59361   \n",
       "unique                  21                 13                           5   \n",
       "top                 FRIEND                  M                          SO   \n",
       "freq                 24896              30724                       28056   \n",
       "\n",
       "       CLNT_JOB_POSITION_TYPE CLNT_JOB_POSITION APP_DRIVING_LICENSE  \\\n",
       "count                   44781            210811               57257   \n",
       "unique                      4             19588                   2   \n",
       "top                SPECIALIST          ДИРЕКТОР                   N   \n",
       "freq                    25123             11200               36332   \n",
       "\n",
       "       APP_EDUCATION APP_TRAVEL_PASS APP_CAR APP_POSITION_TYPE APP_EMP_TYPE  \\\n",
       "count          68104           57257   57256             60545        67362   \n",
       "unique            17               2       2                 4            4   \n",
       "top                H               N       N        SPECIALIST      PRIVATE   \n",
       "freq           42459           52750   32843             36622        59087   \n",
       "\n",
       "       APP_COMP_TYPE    PACK  \n",
       "count          67362  355190  \n",
       "unique             4      12  \n",
       "top          PRIVATE     102  \n",
       "freq           59087  116986  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Missing Values:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CR_PROD_CNT_IL</th>\n",
       "      <th>AMOUNT_RUB_CLO_PRC</th>\n",
       "      <th>PRC_ACCEPTS_A_EMAIL_LINK</th>\n",
       "      <th>APP_REGISTR_RGN_CODE</th>\n",
       "      <th>PRC_ACCEPTS_A_POS</th>\n",
       "      <th>PRC_ACCEPTS_A_TK</th>\n",
       "      <th>TURNOVER_DYNAMIC_IL_1M</th>\n",
       "      <th>CNT_TRAN_AUT_TENDENCY1M</th>\n",
       "      <th>SUM_TRAN_AUT_TENDENCY1M</th>\n",
       "      <th>AMOUNT_RUB_SUP_PRC</th>\n",
       "      <th>...</th>\n",
       "      <th>REST_DYNAMIC_CC_3M</th>\n",
       "      <th>MED_DEBT_PRC_YWZ</th>\n",
       "      <th>LDEAL_ACT_DAYS_PCT_TR3</th>\n",
       "      <th>LDEAL_ACT_DAYS_PCT_AAVG</th>\n",
       "      <th>LDEAL_DELINQ_PER_MAXYWZ</th>\n",
       "      <th>TURNOVER_DYNAMIC_CC_3M</th>\n",
       "      <th>LDEAL_ACT_DAYS_PCT_TR</th>\n",
       "      <th>LDEAL_ACT_DAYS_PCT_TR4</th>\n",
       "      <th>LDEAL_ACT_DAYS_PCT_CURR</th>\n",
       "      <th>TARGET</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Missing Values</th>\n",
       "      <td>0.0</td>\n",
       "      <td>38323.000000</td>\n",
       "      <td>200027.000000</td>\n",
       "      <td>294640.000000</td>\n",
       "      <td>200027.000000</td>\n",
       "      <td>200027.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>278078.000000</td>\n",
       "      <td>278078.000000</td>\n",
       "      <td>38323.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>259477.000000</td>\n",
       "      <td>261742.000000</td>\n",
       "      <td>257015.000000</td>\n",
       "      <td>259477.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>261742.000000</td>\n",
       "      <td>261742.000000</td>\n",
       "      <td>261742.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Percentage</th>\n",
       "      <td>0.0</td>\n",
       "      <td>10.789437</td>\n",
       "      <td>56.315493</td>\n",
       "      <td>82.952786</td>\n",
       "      <td>56.315493</td>\n",
       "      <td>56.315493</td>\n",
       "      <td>0.0</td>\n",
       "      <td>78.289929</td>\n",
       "      <td>78.289929</td>\n",
       "      <td>10.789437</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>73.053014</td>\n",
       "      <td>73.690701</td>\n",
       "      <td>72.359864</td>\n",
       "      <td>73.053014</td>\n",
       "      <td>0.0</td>\n",
       "      <td>73.690701</td>\n",
       "      <td>73.690701</td>\n",
       "      <td>73.690701</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 115 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                CR_PROD_CNT_IL  AMOUNT_RUB_CLO_PRC  PRC_ACCEPTS_A_EMAIL_LINK  \\\n",
       "Missing Values             0.0        38323.000000             200027.000000   \n",
       "Percentage                 0.0           10.789437                 56.315493   \n",
       "\n",
       "                APP_REGISTR_RGN_CODE  PRC_ACCEPTS_A_POS  PRC_ACCEPTS_A_TK  \\\n",
       "Missing Values         294640.000000      200027.000000     200027.000000   \n",
       "Percentage                 82.952786          56.315493         56.315493   \n",
       "\n",
       "                TURNOVER_DYNAMIC_IL_1M  CNT_TRAN_AUT_TENDENCY1M  \\\n",
       "Missing Values                     0.0            278078.000000   \n",
       "Percentage                         0.0                78.289929   \n",
       "\n",
       "                SUM_TRAN_AUT_TENDENCY1M  AMOUNT_RUB_SUP_PRC  ...  \\\n",
       "Missing Values            278078.000000        38323.000000  ...   \n",
       "Percentage                    78.289929           10.789437  ...   \n",
       "\n",
       "                REST_DYNAMIC_CC_3M  MED_DEBT_PRC_YWZ  LDEAL_ACT_DAYS_PCT_TR3  \\\n",
       "Missing Values                 0.0     259477.000000           261742.000000   \n",
       "Percentage                     0.0         73.053014               73.690701   \n",
       "\n",
       "                LDEAL_ACT_DAYS_PCT_AAVG  LDEAL_DELINQ_PER_MAXYWZ  \\\n",
       "Missing Values            257015.000000            259477.000000   \n",
       "Percentage                    72.359864                73.053014   \n",
       "\n",
       "                TURNOVER_DYNAMIC_CC_3M  LDEAL_ACT_DAYS_PCT_TR  \\\n",
       "Missing Values                     0.0          261742.000000   \n",
       "Percentage                         0.0              73.690701   \n",
       "\n",
       "                LDEAL_ACT_DAYS_PCT_TR4  LDEAL_ACT_DAYS_PCT_CURR  TARGET  \n",
       "Missing Values           261742.000000            261742.000000     0.0  \n",
       "Percentage                   73.690701                73.690701     0.0  \n",
       "\n",
       "[2 rows x 115 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TARGET\n",
      "0    326265\n",
      "1     28925\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "def explore_data(df):\n",
    "    \"\"\"Display basic information about the DataFrame and drop 'ID' if present. Returns the (possibly) modified DataFrame.\"\"\"\n",
    "    print(\"DataFrame Info:\")\n",
    "    df.info()\n",
    "    \n",
    "    # drop unnecessary column if it exists\n",
    "    if 'ID' in df.columns:\n",
    "        df = df.drop(columns=['ID'])\n",
    "        print(\"\\nDropped column: 'ID'\")\n",
    "    else:\n",
    "        print(\"\\nColumn 'ID' not found; skipping drop.\")\n",
    "    \n",
    "    print(\"\\nDataFrame Description (numeric):\")\n",
    "    display(df.describe())\n",
    "    \n",
    "    print(\"\\nDataFrame Description (object):\")\n",
    "    display(df.describe(include=['object']))\n",
    "    \n",
    "    # optionally show missing values and plot in grid format\n",
    "    print(\"\\nMissing Values:\")\n",
    "    df_info = pd.DataFrame(df.isnull().sum(), columns=['Missing Values'])\n",
    "    df_info['Percentage'] = (df_info['Missing Values'] / len(df)) * 100\n",
    "\n",
    "    #siwich column into rows for better display\n",
    "\n",
    "    df_info = df_info.transpose()\n",
    "    display(df_info)\n",
    "    # display(df.isnull().sum())\n",
    "    \n",
    "    # check the distribution of target column\n",
    "    print(df['TARGET'].value_counts())\n",
    "    return df\n",
    "\n",
    "df = explore_data(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f3232174",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.918564711844365"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "326265/df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "24795fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import chi2_contingency\n",
    "import numpy as np\n",
    "\n",
    "def cramers_v(x, y):\n",
    "    \"\"\"Measure association between categorical x and binary y (TARGET)\"\"\"\n",
    "    confusion_matrix = pd.crosstab(x, y)\n",
    "    chi2 = chi2_contingency(confusion_matrix)[0]\n",
    "    n = confusion_matrix.sum().sum()\n",
    "    phi2 = chi2 / n\n",
    "    r, k = confusion_matrix.shape\n",
    "    return np.sqrt(phi2 / min(k-1, r-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "76f00f20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Analyzing columns with >70% missing values for correlation with TARGET...\n",
      "df shape before dropping columns: (355190, 115)\n",
      "Dropping column APP_REGISTR_RGN_CODE due to low correlation (-0.028) with TARGET\n",
      "Dropping column CLNT_TRUST_RELATION due to low correlation (0.029) with TARGET\n",
      "Dropping column APP_MARITAL_STATUS due to low correlation (0.030) with TARGET\n",
      "Dropping column APP_KIND_OF_PROP_HABITATION due to low correlation (0.008) with TARGET\n",
      "Dropping column CLNT_JOB_POSITION_TYPE due to low correlation (0.036) with TARGET\n",
      "Dropping column APP_DRIVING_LICENSE due to low correlation (0.031) with TARGET\n",
      "Dropping column APP_EDUCATION due to low correlation (0.058) with TARGET\n",
      "Dropping column APP_TRAVEL_PASS due to low correlation (0.024) with TARGET\n",
      "Dropping column APP_CAR due to low correlation (0.028) with TARGET\n",
      "Dropping column APP_POSITION_TYPE due to low correlation (0.034) with TARGET\n",
      "Dropping column APP_EMP_TYPE due to low correlation (0.030) with TARGET\n",
      "Dropping column DEAL_YQZ_IR_MAX due to low correlation (0.019) with TARGET\n",
      "Dropping column LDEAL_YQZ_COM due to low correlation (-0.028) with TARGET\n",
      "Dropping column DEAL_YQZ_IR_MIN due to low correlation (0.024) with TARGET\n",
      "Dropping column LDEAL_TENOR_MIN due to low correlation (0.024) with TARGET\n",
      "Dropping column APP_COMP_TYPE due to low correlation (0.030) with TARGET\n",
      "Dropping column DEAL_GRACE_DAYS_ACC_S1X1 due to low correlation (-0.003) with TARGET\n",
      "Dropping column AVG_PCT_MONTH_TO_PCLOSE due to low correlation (0.009) with TARGET\n",
      "Dropping column DEAL_YWZ_IR_MIN due to low correlation (-0.016) with TARGET\n",
      "Dropping column DEAL_YWZ_IR_MAX due to low correlation (-0.039) with TARGET\n",
      "Dropping column DEAL_GRACE_DAYS_ACC_AVG due to low correlation (-0.010) with TARGET\n",
      "Dropping column LDEAL_YQZ_PC due to low correlation (0.023) with TARGET\n",
      "Dropping column DEAL_GRACE_DAYS_ACC_MAX due to low correlation (-0.011) with TARGET\n",
      "Dropping column CLNT_SALARY_VALUE due to low correlation (0.012) with TARGET\n",
      "Dropping column LDEAL_USED_AMT_AVG_YQZ due to low correlation (-0.008) with TARGET\n",
      "Dropping column LDEAL_USED_AMT_AVG_YWZ due to low correlation (-0.013) with TARGET\n",
      "Dropping column LDEAL_DELINQ_PER_MAXYWZ due to low correlation (0.001) with TARGET\n",
      "df shape after dropping columns: (355190, 88)\n"
     ]
    }
   ],
   "source": [
    "# Annalyze data whitch to keep or drop\n",
    "print(\"\\nAnalyzing columns with >70% missing values for correlation with TARGET...\")\n",
    "print(f\"df shape before dropping columns: {df.shape}\")\n",
    "high_missing = df.columns[df.isnull().sum() / len(df) > TRESHOLD]\n",
    "for col in high_missing:\n",
    "    if col == 'TARGET':\n",
    "        continue\n",
    "    # coerce to numeric where possible, compute correlation with TARGET\n",
    "    \n",
    "    if df[col].dtype == 'object':\n",
    "        # categorical column\n",
    "        corr = cramers_v(df[col].dropna(), df.loc[df[col].notnull(), 'TARGET'])\n",
    "        if corr < 0.1:\n",
    "            print(f\"Dropping column {col} due to low correlation ({corr:.3f}) with TARGET\")\n",
    "            df = df.drop(columns=[col])\n",
    "    else:\n",
    "        # numerical column\n",
    "        series_num = pd.to_numeric(df[col], errors='coerce')\n",
    "        corr = series_num.corr(df['TARGET'])\n",
    "        if pd.notnull(corr) and abs(corr) < 0.05:\n",
    "            print(f\"Dropping column {col} due to low correlation ({corr:.3f}) with TARGET\")\n",
    "            df = df.drop(columns=[col])\n",
    "    \n",
    "    \n",
    "    # series_num = pd.to_numeric(df[col], errors='coerce')\n",
    "    \n",
    "    \n",
    "    \n",
    "    # corr = cramers_v(df[col].dropna(), df.loc[df[col].notnull(), 'TARGET'])\n",
    "    # missing_pct = df[col].isnull().sum() / len(df) * 100\n",
    "    # corr_str = f\"{corr:.3f}\" if pd.notnull(corr) else \"N/A\"\n",
    "    # print(f\"{col}: Missing={missing_pct:.1f}%, Correlation={corr_str}\")\n",
    "print(f\"df shape after dropping columns: {df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "52a1da3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLNT_JOB_POSITION: object\n",
      "PACK: object\n"
     ]
    }
   ],
   "source": [
    "for col in df:\n",
    "    column_type = df[col].dtype\n",
    "    if column_type != 'int64' and column_type != 'float64':\n",
    "        print(f\"{col}: {column_type}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "37034e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def normalize_categorical_columns(df):\n",
    "    \"\"\"\n",
    "    Normalize categorical columns by converting to lowercase/uppercase\n",
    "    and stripping whitespace\n",
    "    \"\"\"\n",
    "    cat_cols = df.select_dtypes(include=['object']).columns.tolist()\n",
    "    \n",
    "    for col in cat_cols:\n",
    "        # Convert to string, lowercase, and strip whitespace\n",
    "        df[col] = df[col].astype(str).str.upper()\n",
    "        df[col] = df[col].str.strip()\n",
    "        df[col] = df[col].replace('NAN', np.nan)  # Replace 'NA' strings with NaN\n",
    "        df[col] = df[col].replace('', np.nan)  # Replace empty strings with NaN\n",
    "    \n",
    "    return df\n",
    "df = normalize_categorical_columns(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8d153efe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLNT_JOB_POSITION: object\n",
      "PACK: object\n",
      "PACK: Missing=0.0%, Correlation=0.071\n",
      "existing_values length: 12\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'K01': 77083,\n",
       " '102': 116986,\n",
       " '105': 44936,\n",
       " 'O01': 50478,\n",
       " '103': 24860,\n",
       " '101': 1816,\n",
       " '107': 27952,\n",
       " '301': 4208,\n",
       " '104': 6776,\n",
       " '108': 2,\n",
       " '109': 86,\n",
       " 'M01': 7}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for col in df:\n",
    "    column_type = df[col].dtype\n",
    "    if column_type != 'int64' and column_type != 'float64':\n",
    "        print(f\"{col}: {column_type}\")\n",
    "\n",
    "\n",
    "# Count non-null occurrences of each job position\n",
    "existing_values = {}\n",
    "for val in df['PACK']:\n",
    "    if pd.isnull(val):\n",
    "        continue\n",
    "    existing_values[val] = existing_values.get(val, 0) + 1\n",
    "corr = cramers_v(df['PACK'].dropna(), df.loc[df['PACK'].notnull(), 'TARGET'])\n",
    "missing_pct = df['PACK'].isnull().sum() / len(df) * 100\n",
    "print(f\"PACK: Missing={missing_pct:.1f}%, Correlation={corr:.3f}\")\n",
    "print(f\"existing_values length: {len(existing_values)}\")\n",
    "existing_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2ac3a7c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_encoders(X_train, y_train):\n",
    "    \"\"\"\n",
    "    Fit encoders on training data and return fitted encoders.\n",
    "    \"\"\"\n",
    "    # Get object columns\n",
    "    obj_cols = X_train.select_dtypes(include=['object']).columns.tolist()\n",
    "    \n",
    "    # Try numeric conversion\n",
    "    numeric_conversions = {}\n",
    "    for col in obj_cols:\n",
    "        converted = pd.to_numeric(X_train[col], errors='coerce')\n",
    "        if converted.notna().mean() > 0.9:\n",
    "            numeric_conversions[col] = True\n",
    "    print(\"numeric conversion\", numeric_conversions)\n",
    "    # Update object columns\n",
    "    obj_cols = [col for col in obj_cols if col not in numeric_conversions]\n",
    "    \n",
    "    # Categorize by cardinality\n",
    "    low_card_cols = []\n",
    "    high_card_cols = []\n",
    "    \n",
    "    for col in obj_cols:\n",
    "        if X_train[col].isnull().all():\n",
    "            continue\n",
    "        \n",
    "        unique_count = X_train[col].nunique(dropna=True)\n",
    "        \n",
    "        if unique_count < 10:\n",
    "            low_card_cols.append(col)\n",
    "        else:\n",
    "            high_card_cols.append(col)\n",
    "    \n",
    "    # Fit target encoder on training data\n",
    "    target_encoder = None\n",
    "    if high_card_cols:\n",
    "        target_encoder = ce.TargetEncoder(cols=high_card_cols, smoothing=1.0)\n",
    "        valid_mask = X_train[high_card_cols].notna().all(axis=1) & y_train['TARGET'].notna()\n",
    "        target_encoder.fit(X_train.loc[valid_mask, high_card_cols], y_train.loc[valid_mask, 'TARGET'])\n",
    "    \n",
    "    return {\n",
    "        'numeric_conversions': numeric_conversions,\n",
    "        'low_card_cols': low_card_cols,\n",
    "        'high_card_cols': high_card_cols,\n",
    "        'target_encoder': target_encoder,\n",
    "        'global_mean': y_train['TARGET'].mean()\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "85945e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def transform_with_encoders(X, encoders_dict):\n",
    "    \"\"\"\n",
    "    Transform data using pre-fitted encoders.\n",
    "    \"\"\"\n",
    "    X = X.copy()\n",
    "    \n",
    "    # Apply numeric conversions\n",
    "    for col in encoders_dict['numeric_conversions']:\n",
    "        X[col] = pd.to_numeric(X[col], errors='coerce')\n",
    "    \n",
    "    # Drop all-null columns\n",
    "    obj_cols = X.select_dtypes(include=['object']).columns\n",
    "    for col in obj_cols:\n",
    "        if X[col].isnull().all():\n",
    "            X = X.drop(columns=[col])\n",
    "    \n",
    "    # Target encode high-cardinality columns\n",
    "    if encoders_dict['high_card_cols'] and encoders_dict['target_encoder']:\n",
    "        high_card_cols = encoders_dict['high_card_cols']\n",
    "        print(f\"\\n\\nTarget encoding: {high_card_cols}\\n\\n\")\n",
    "        for col in high_card_cols:\n",
    "            if col in X.columns:\n",
    "                # Transform using fitted encoder\n",
    "                print(f\"Encoding column: {col}\")\n",
    "                print()\n",
    "                try:\n",
    "                    # Transform using the encoder fitted on all high-cardinality cols,\n",
    "                    # then take the encoded series for the current column\n",
    "                    encoded = encoders_dict['target_encoder'].transform(X[encoders_dict['high_card_cols']])[col]\n",
    "                    X[col] = encoded\n",
    "                    # Fill unseen / missing encodings with global mean\n",
    "                    X[col].fillna(encoders_dict['global_mean'], inplace=True)\n",
    "                except Exception:\n",
    "                    # If transform fails, fall back to global mean\n",
    "                    X[col] = encoders_dict['global_mean']\n",
    "        #         X = X.drop(columns=[col])\n",
    "                \n",
    "                \n",
    "                \n",
    "        #         X[f\"{col}\"] = encoded[col]\n",
    "                \n",
    "        #         # Fill nulls with global mean\n",
    "        #         X[f\"{col}\"].fillna(encoders_dict['global_mean'], inplace=True)\n",
    "        \n",
    "        # Drop original columns\n",
    "        # X = X.drop(columns=[col for col in high_card_cols if col in X.columns])\n",
    "    \n",
    "    # One-hot encode low-cardinality columns\n",
    "    if encoders_dict['low_card_cols']:\n",
    "        print(f\"\\nOne-hot encoding: {encoders_dict['low_card_cols']}\")\n",
    "\n",
    "        X = pd.get_dummies(X, columns=encoders_dict['low_card_cols'], drop_first=True)\n",
    "    \n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d72a35ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (284152, 87), Test shape: (71038, 87)\n"
     ]
    }
   ],
   "source": [
    "# split data train test\n",
    "\n",
    "\n",
    "y = pd.DataFrame()\n",
    "y['TARGET'] = df['TARGET']\n",
    "X = df.drop(columns=['TARGET'])\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "                            X,y,\n",
    "                            test_size=0.2,\n",
    "                            random_state=42,\n",
    "                            stratify=y)\n",
    "\n",
    "print(f\"Train shape: {X_train.shape}, Test shape: {X_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d69295e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting encoders on training data...\n",
      "================================================================================\n",
      "numeric conversion {}\n",
      "numeric_conversions: {}\n",
      "low_card_cols: []\n",
      "high_card_cols: ['CLNT_JOB_POSITION', 'PACK']\n",
      "target_encoder: TargetEncoder(cols=['CLNT_JOB_POSITION', 'PACK'], smoothing=1.0)\n",
      "global_mean: 0.08143528815563501\n"
     ]
    }
   ],
   "source": [
    "# Fit encoders on TRAINING data only\n",
    "print(\"Fitting encoders on training data...\")\n",
    "print(\"=\"*80)\n",
    "encoders = fit_encoders(X_train, y_train)\n",
    "for key, value in encoders.items():\n",
    "        print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "05db1825",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Transforming training data...\n",
      "\n",
      "\n",
      "Target encoding: ['CLNT_JOB_POSITION', 'PACK']\n",
      "\n",
      "\n",
      "Encoding column: CLNT_JOB_POSITION\n",
      "\n",
      "Encoding column: PACK\n",
      "\n",
      "\n",
      "Transforming test data...\n",
      "\n",
      "\n",
      "Target encoding: ['CLNT_JOB_POSITION', 'PACK']\n",
      "\n",
      "\n",
      "Encoding column: CLNT_JOB_POSITION\n",
      "\n",
      "Encoding column: PACK\n",
      "\n",
      "\n",
      "After encoding - Train: (284152, 87), Test: (71038, 87)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_12578/204901964.py:32: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X[col].fillna(encoders_dict['global_mean'], inplace=True)\n",
      "/tmp/ipykernel_12578/204901964.py:32: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X[col].fillna(encoders_dict['global_mean'], inplace=True)\n",
      "/tmp/ipykernel_12578/204901964.py:32: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X[col].fillna(encoders_dict['global_mean'], inplace=True)\n",
      "/tmp/ipykernel_12578/204901964.py:32: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X[col].fillna(encoders_dict['global_mean'], inplace=True)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nTransforming training data...\")\n",
    "X_train_encoded = transform_with_encoders(X_train, encoders)\n",
    "\n",
    "print(\"\\nTransforming test data...\")\n",
    "X_test_encoded = transform_with_encoders(X_test, encoders)\n",
    "\n",
    "\n",
    "print(f\"\\nAfter encoding - Train: {X_train_encoded.shape}, Test: {X_test_encoded.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "721347ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Aligning train/test columns...\n"
     ]
    }
   ],
   "source": [
    "# Just to make sure not needed in our case -------------------------------------\n",
    "# Align columns (ensure train and test have same columns)\n",
    "print(\"\\nAligning train/test columns...\")\n",
    "train_cols = set(X_train_encoded.columns)\n",
    "test_cols = set(X_test_encoded.columns)\n",
    "\n",
    "# Add missing columns to test (fill with 0)\n",
    "for col in train_cols - test_cols:\n",
    "    X_test_encoded[col] = 0\n",
    "    print(f\"Added missing column to test: {col}\")\n",
    "\n",
    "# Remove extra columns from test\n",
    "for col in test_cols - train_cols:\n",
    "    X_test_encoded = X_test_encoded.drop(columns=[col])\n",
    "    print(f\"Removed extra column from test: {col}\")\n",
    "#-------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "282f6db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reorder test columns to match train\n",
    "\n",
    "#! Mandatory step to ensure columns are in the same order after encoding\n",
    "\n",
    "X_test_encoded = X_test_encoded[X_train_encoded.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c4e37337",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train nulls: 8813805\n",
      "Test nulls: 2202381\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nTrain nulls: {X_train_encoded.isnull().sum().sum()}\")\n",
    "print(f\"Test nulls: {X_test_encoded.isnull().sum().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bdd56fbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "After fillna with mean\n",
      "Train nulls: 0\n",
      "Test nulls: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_12578/4282412580.py:5: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X_train_encoded[col].fillna(imputation_values[col], inplace=True)\n",
      "/tmp/ipykernel_12578/4282412580.py:8: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X_test_encoded[col].fillna(imputation_values[col], inplace=True)\n"
     ]
    }
   ],
   "source": [
    "imputation_values = X_train_encoded.mean()\n",
    "\n",
    "for col in X_train_encoded.columns:\n",
    "    if X_train_encoded[col].isnull().any():\n",
    "        X_train_encoded[col].fillna(imputation_values[col], inplace=True)\n",
    "    \n",
    "    if X_test_encoded[col].isnull().any():\n",
    "        X_test_encoded[col].fillna(imputation_values[col], inplace=True)\n",
    "print(f\"\\nAfter fillna with mean\\nTrain nulls: {X_train_encoded.isnull().sum().sum()}\")\n",
    "print(f\"Test nulls: {X_test_encoded.isnull().sum().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5a7f235e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balancing training set using SMOTE...\n",
      "Before SMOTE: TARGET\n",
      "0    261012\n",
      "1     23140\n",
      "Name: count, dtype: int64 | X shape: (284152, 87)\n",
      "After SMOTE: TARGET\n",
      "0    261012\n",
      "1    261012\n",
      "Name: count, dtype: int64 | X shape: (522024, 87)\n"
     ]
    }
   ],
   "source": [
    "# Balence Dataset for training set only\n",
    "\n",
    "print(\"Balancing training set using SMOTE...\")\n",
    "print(f\"Before SMOTE: {y_train['TARGET'].value_counts()} | X shape: {X_train_encoded.shape}\")\n",
    "smote = SMOTE(sampling_strategy='minority', random_state=42)\n",
    "X_train_balanced, y_train_balanced = smote.fit_resample(X_train_encoded, y_train)\n",
    "print(f\"After SMOTE: {y_train_balanced['TARGET'].value_counts()} | X shape: {X_train_balanced.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6d8a5ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standerdize\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train_balanced = pd.DataFrame(scaler.fit_transform(X_train_balanced), columns=X_train_balanced.columns)\n",
    "X_test_encoded = pd.DataFrame(scaler.transform(X_test_encoded), columns=X_test_encoded.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72ec403b",
   "metadata": {},
   "source": [
    "# Train with Basline Classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7a009d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def baseline_model(X_train, y_train, X_test, y_test):\n",
    "    from sklearn.dummy import DummyClassifier\n",
    "    from sklearn.metrics import classification_report, confusion_matrix\n",
    "    \n",
    "    dummy_clf = DummyClassifier(strategy='most_frequent')\n",
    "    dummy_clf.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred = dummy_clf.predict(X_test)\n",
    "    \n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "    \n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    return dummy_clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f3ec66b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[65253     0]\n",
      " [ 5785     0]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      1.00      0.96     65253\n",
      "           1       0.00      0.00      0.00      5785\n",
      "\n",
      "    accuracy                           0.92     71038\n",
      "   macro avg       0.46      0.50      0.48     71038\n",
      "weighted avg       0.84      0.92      0.88     71038\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hasabir/miniconda3/envs/ai/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/home/hasabir/miniconda3/envs/ai/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/home/hasabir/miniconda3/envs/ai/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    }
   ],
   "source": [
    "dummy_clf =  baseline_model(X_train_balanced, y_train_balanced, X_test_encoded, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12d66deb",
   "metadata": {},
   "source": [
    "# Train with Random Forest + Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8694341a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x train shape: (522024, 87), y train shape: (522024, 1)\n",
      "x test shape: (71038, 87), y test shape: (71038, 1)\n"
     ]
    }
   ],
   "source": [
    "print(f\"x train shape: {X_train_balanced.shape}, y train shape: {y_train_balanced.shape}\")\n",
    "print(f\"x test shape: {X_test_encoded.shape}, y test shape: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "11cf8fdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hasabir/miniconda3/envs/ai/lib/python3.12/site-packages/sklearn/base.py:1365: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/home/hasabir/miniconda3/envs/ai/lib/python3.12/site-packages/sklearn/base.py:1365: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/home/hasabir/miniconda3/envs/ai/lib/python3.12/site-packages/sklearn/base.py:1365: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/home/hasabir/miniconda3/envs/ai/lib/python3.12/site-packages/sklearn/base.py:1365: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/home/hasabir/miniconda3/envs/ai/lib/python3.12/site-packages/sklearn/base.py:1365: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/home/hasabir/miniconda3/envs/ai/lib/python3.12/site-packages/sklearn/base.py:1365: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/home/hasabir/miniconda3/envs/ai/lib/python3.12/site-packages/sklearn/base.py:1365: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/home/hasabir/miniconda3/envs/ai/lib/python3.12/site-packages/sklearn/base.py:1365: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/home/hasabir/miniconda3/envs/ai/lib/python3.12/site-packages/sklearn/base.py:1365: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/home/hasabir/miniconda3/envs/ai/lib/python3.12/site-packages/sklearn/base.py:1365: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/home/hasabir/miniconda3/envs/ai/lib/python3.12/site-packages/sklearn/base.py:1365: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/home/hasabir/miniconda3/envs/ai/lib/python3.12/site-packages/sklearn/base.py:1365: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END max_depth=20, min_samples_split=10, n_estimators=100; total time= 2.5min\n",
      "[CV] END max_depth=20, min_samples_split=2, n_estimators=100; total time= 2.8min\n",
      "[CV] END max_depth=20, min_samples_split=10, n_estimators=100; total time= 3.0min\n",
      "[CV] END max_depth=20, min_samples_split=10, n_estimators=100; total time= 3.0min\n",
      "[CV] END max_depth=None, min_samples_split=2, n_estimators=100; total time= 3.1min\n",
      "[CV] END max_depth=20, min_samples_split=2, n_estimators=100; total time= 3.2min\n",
      "[CV] END max_depth=20, min_samples_split=2, n_estimators=100; total time= 3.2min\n",
      "[CV] END max_depth=None, min_samples_split=2, n_estimators=100; total time= 3.2min\n",
      "[CV] END max_depth=None, min_samples_split=10, n_estimators=100; total time= 3.3min\n",
      "[CV] END max_depth=None, min_samples_split=10, n_estimators=100; total time= 3.4min\n",
      "[CV] END max_depth=None, min_samples_split=10, n_estimators=100; total time= 3.4min\n",
      "[CV] END max_depth=None, min_samples_split=2, n_estimators=100; total time= 3.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hasabir/miniconda3/envs/ai/lib/python3.12/site-packages/sklearn/base.py:1365: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters found: {'max_depth': None, 'min_samples_split': 2, 'n_estimators': 100}\n",
      "Best CV score: 0.9372\n",
      "Test set accuracy: 0.9043\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# param_grid = {\n",
    "#     'n_estimators': [50, 100],           # Reduced from 3 to 2\n",
    "#     'max_depth': [None, 20],             # Reduced from 4 to 2\n",
    "#     'min_samples_split': [2, 10],        # Reduced from 3 to 2\n",
    "# }\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [100],              # 100 is usually good enough\n",
    "    'max_depth': [None, 20],            \n",
    "    'min_samples_split': [2, 10],\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "def random_forest_model(X_train, y_train, X_test, y_test):\n",
    "    # Initialize model\n",
    "    rf_clf = RandomForestClassifier(random_state=42)\n",
    "    \n",
    "    # GridSearch with cross-validation\n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=rf_clf,\n",
    "        param_grid=param_grid,\n",
    "        cv=3, # \n",
    "        scoring='accuracy',  # Consider 'f1', 'roc_auc', or 'balanced_accuracy' for imbalanced data\n",
    "        n_jobs=-1,\n",
    "        verbose=2 # Increased verbosity for more detailed output\n",
    "    )\n",
    "    \n",
    "    # Fit - handle y_train whether it's Series or array\n",
    "    grid_search.fit(X_train, y_train.ravel() if hasattr(y_train, 'ravel') else y_train)\n",
    "    \n",
    "    # Print best parameters\n",
    "    print(f\"Best parameters found: {grid_search.best_params_}\")\n",
    "    print(f\"Best CV score: {grid_search.best_score_:.4f}\")\n",
    "    \n",
    "    # Get best model\n",
    "    best_rf_model = grid_search.best_estimator_\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    test_accuracy = best_rf_model.score(X_test, y_test)\n",
    "    print(f\"Test set accuracy: {test_accuracy:.4f}\")\n",
    "    \n",
    "    # Return the trained model for later use\n",
    "    return best_rf_model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Train the model\n",
    "best_rf_model = random_forest_model(X_train_balanced, y_train_balanced, \n",
    "                                  X_test_encoded, y_test)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f476192e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[63576  1677]\n",
      " [ 5124   661]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Test the model\n",
    "\n",
    "y_pred = best_rf_model.predict(X_test_encoded)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d58001",
   "metadata": {},
   "source": [
    "# Train with Scikit-learn MLPClassifier + Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6f17e91a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hasabir/miniconda3/envs/ai/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:781: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "def sckit_learn_mlp_model(X_train, y_train, X_test, y_test):\n",
    "    from sklearn.neural_network import MLPClassifier\n",
    "    from sklearn.metrics import classification_report, confusion_matrix\n",
    "    \n",
    "    mlp_clf = MLPClassifier(hidden_layer_sizes=(100, ), max_iter=300, random_state=42) #solver='adam' by default we can tune more parameters later with sgd, learning_rate, etc.\n",
    "    mlp_clf.fit(X_train, y_train.values.ravel())\n",
    "    \n",
    "    return mlp_clf\n",
    "mlp_model =  sckit_learn_mlp_model(X_train_balanced, y_train_balanced, X_test_encoded, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a6e88c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# param_grid = {\n",
    "#     'hidden_layer_sizes': [(50,), (50, 50)],\n",
    "#     'activation': ['relu', 'tanh'],\n",
    "#     'solver': ['adam'],\n",
    "#     'alpha': [0.001],\n",
    "#     'learning_rate_init': [0.01],\n",
    "#     'max_iter': [300]\n",
    "# }\n",
    "\n",
    "\n",
    "\n",
    "param_grid = {\n",
    "    'hidden_layer_sizes': [(50,), (100,), (50, 50)],\n",
    "    'activation': ['relu', 'tanh'],\n",
    "    'solver': ['adam'],\n",
    "    'alpha': [0.0001, 0.001],\n",
    "    'learning_rate_init': [0.001, 0.01],\n",
    "    'max_iter': [300]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd9ac14e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hasabir/miniconda3/envs/ai/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:1219: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/hasabir/miniconda3/envs/ai/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:1219: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/hasabir/miniconda3/envs/ai/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:1219: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/hasabir/miniconda3/envs/ai/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:1219: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/hasabir/miniconda3/envs/ai/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:1219: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/hasabir/miniconda3/envs/ai/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:1219: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/hasabir/miniconda3/envs/ai/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:1219: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/hasabir/miniconda3/envs/ai/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:1219: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/hasabir/miniconda3/envs/ai/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:1219: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/hasabir/miniconda3/envs/ai/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:1219: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/hasabir/miniconda3/envs/ai/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:1219: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/hasabir/miniconda3/envs/ai/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:1219: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END activation=tanh, alpha=0.001, hidden_layer_sizes=(50,), learning_rate_init=0.01, max_iter=300, solver=adam; total time= 1.5min\n",
      "[CV] END activation=tanh, alpha=0.001, hidden_layer_sizes=(50, 50), learning_rate_init=0.01, max_iter=300, solver=adam; total time= 2.1min\n",
      "[CV] END activation=tanh, alpha=0.001, hidden_layer_sizes=(50,), learning_rate_init=0.01, max_iter=300, solver=adam; total time= 2.1min\n",
      "[CV] END activation=tanh, alpha=0.001, hidden_layer_sizes=(50,), learning_rate_init=0.01, max_iter=300, solver=adam; total time= 2.9min\n",
      "[CV] END activation=tanh, alpha=0.001, hidden_layer_sizes=(50, 50), learning_rate_init=0.01, max_iter=300, solver=adam; total time= 3.0min\n",
      "[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(50,), learning_rate_init=0.01, max_iter=300, solver=adam; total time= 3.6min\n",
      "[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(50,), learning_rate_init=0.01, max_iter=300, solver=adam; total time= 3.7min\n",
      "[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(50,), learning_rate_init=0.01, max_iter=300, solver=adam; total time= 4.5min\n",
      "[CV] END activation=tanh, alpha=0.001, hidden_layer_sizes=(50, 50), learning_rate_init=0.01, max_iter=300, solver=adam; total time= 4.8min\n",
      "[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(50, 50), learning_rate_init=0.01, max_iter=300, solver=adam; total time= 5.7min\n",
      "[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(50, 50), learning_rate_init=0.01, max_iter=300, solver=adam; total time= 5.8min\n",
      "[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(50, 50), learning_rate_init=0.01, max_iter=300, solver=adam; total time=10.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hasabir/miniconda3/envs/ai/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:1219: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "\n",
    "def scikit_learn_mlp_model(X_train, y_train):\n",
    "    mlp_clf = MLPClassifier(random_state=42)\n",
    "\n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=mlp_clf,\n",
    "        param_grid=param_grid,\n",
    "        cv=3,\n",
    "        scoring='accuracy',\n",
    "        n_jobs=-1,\n",
    "        verbose=2\n",
    "    )\n",
    "\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    return grid_search.best_estimator_\n",
    "\n",
    "mlp_model = scikit_learn_mlp_model(X_train_balanced, y_train_balanced)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf2f0341",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mlp_model = scikit_learn_mlp_model(X_train_balanced, y_train_balanced)\n",
    "mlp_grid_model = scikit_learn_mlp_model(X_train_balanced, y_train_balanced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "74872013",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[59494  5759]\n",
      " [ 4022  1763]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.91      0.92     65253\n",
      "           1       0.23      0.30      0.26      5785\n",
      "\n",
      "    accuracy                           0.86     71038\n",
      "   macro avg       0.59      0.61      0.59     71038\n",
      "weighted avg       0.88      0.86      0.87     71038\n",
      "\n",
      "Test set accuracy: 0.8623\n",
      "Test set accuracy with Grid Search: 0.8623\n",
      "Test set accuracy with Grid Search 2: 0.8623\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "y_pred = mlp_model.predict(X_test_encoded)\n",
    "    \n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Test set accuracy: {accuracy:.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "accuracy_with_grid_search = mlp_model.score(X_test_encoded, y_test)\n",
    "print(f\"Test set accuracy with Grid Search: {accuracy_with_grid_search:.4f}\")\n",
    "\n",
    "accuracy_with_grid_search2 = mlp_grid_model.score(X_test_encoded, y_test)\n",
    "print(f\"Test set accuracy with Grid Search 2: {accuracy_with_grid_search:.4f}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdb3ae66",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "05970149",
   "metadata": {},
   "source": [
    "# Predict and Test all models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8353b78f",
   "metadata": {},
   "source": [
    "### Load Test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a05e71f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Target encoding: ['CLNT_JOB_POSITION', 'PACK']\n",
      "\n",
      "\n",
      "Encoding column: CLNT_JOB_POSITION\n",
      "\n",
      "Encoding column: PACK\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hasabir/Desktop/projects/churn_42/preprocessing.py:178: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X[col].fillna(encoders_dict['global_mean'], inplace=True)\n",
      "/home/hasabir/Desktop/projects/churn_42/preprocessing.py:178: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X[col].fillna(encoders_dict['global_mean'], inplace=True)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The feature names should match those that were passed during fit.\nFeature names unseen at fit time:\n- APP_CAR\n- APP_COMP_TYPE\n- APP_DRIVING_LICENSE\n- APP_EDUCATION\n- APP_EMP_TYPE\n- ...\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[31]\u001b[39m\u001b[32m, line 17\u001b[39m\n\u001b[32m     14\u001b[39m     predictions = model.predict(df)\n\u001b[32m     15\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m predictions\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m predictions = \u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdummy_clf\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[31]\u001b[39m\u001b[32m, line 11\u001b[39m, in \u001b[36mpredict\u001b[39m\u001b[34m(model)\u001b[39m\n\u001b[32m      7\u001b[39m df_test = preprocessor.transform_with_encoders(df_test, encoders)\n\u001b[32m     10\u001b[39m df_test = preprocessor.normalize_categorical_columns(df_test)\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m df_test_encoded = pd.DataFrame(\u001b[43mscaler\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_test\u001b[49m\u001b[43m)\u001b[49m, columns=df_test.columns)\n\u001b[32m     12\u001b[39m df = preprocessor.fill_missing_with_mean(df_test_encoded, imputation_values)\n\u001b[32m     14\u001b[39m predictions = model.predict(df)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/ai/lib/python3.12/site-packages/sklearn/utils/_set_output.py:316\u001b[39m, in \u001b[36m_wrap_method_output.<locals>.wrapped\u001b[39m\u001b[34m(self, X, *args, **kwargs)\u001b[39m\n\u001b[32m    314\u001b[39m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[32m    315\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, *args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m316\u001b[39m     data_to_wrap = \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    317\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[32m    318\u001b[39m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[32m    319\u001b[39m         return_tuple = (\n\u001b[32m    320\u001b[39m             _wrap_data_with_container(method, data_to_wrap[\u001b[32m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[32m    321\u001b[39m             *data_to_wrap[\u001b[32m1\u001b[39m:],\n\u001b[32m    322\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/ai/lib/python3.12/site-packages/sklearn/preprocessing/_data.py:1075\u001b[39m, in \u001b[36mStandardScaler.transform\u001b[39m\u001b[34m(self, X, copy)\u001b[39m\n\u001b[32m   1072\u001b[39m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[32m   1074\u001b[39m copy = copy \u001b[38;5;28;01mif\u001b[39;00m copy \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.copy\n\u001b[32m-> \u001b[39m\u001b[32m1075\u001b[39m X = \u001b[43mvalidate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1076\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1077\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1078\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreset\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1079\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcsr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1080\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1081\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mFLOAT_DTYPES\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1082\u001b[39m \u001b[43m    \u001b[49m\u001b[43mforce_writeable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1083\u001b[39m \u001b[43m    \u001b[49m\u001b[43mensure_all_finite\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mallow-nan\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1084\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1086\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m sparse.issparse(X):\n\u001b[32m   1087\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.with_mean:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/ai/lib/python3.12/site-packages/sklearn/utils/validation.py:2929\u001b[39m, in \u001b[36mvalidate_data\u001b[39m\u001b[34m(_estimator, X, y, reset, validate_separately, skip_check_array, **check_params)\u001b[39m\n\u001b[32m   2845\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mvalidate_data\u001b[39m(\n\u001b[32m   2846\u001b[39m     _estimator,\n\u001b[32m   2847\u001b[39m     /,\n\u001b[32m   (...)\u001b[39m\u001b[32m   2853\u001b[39m     **check_params,\n\u001b[32m   2854\u001b[39m ):\n\u001b[32m   2855\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Validate input data and set or check feature names and counts of the input.\u001b[39;00m\n\u001b[32m   2856\u001b[39m \n\u001b[32m   2857\u001b[39m \u001b[33;03m    This helper function should be used in an estimator that requires input\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   2927\u001b[39m \u001b[33;03m        validated.\u001b[39;00m\n\u001b[32m   2928\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2929\u001b[39m     \u001b[43m_check_feature_names\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_estimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2930\u001b[39m     tags = get_tags(_estimator)\n\u001b[32m   2931\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m tags.target_tags.required:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/ai/lib/python3.12/site-packages/sklearn/utils/validation.py:2787\u001b[39m, in \u001b[36m_check_feature_names\u001b[39m\u001b[34m(estimator, X, reset)\u001b[39m\n\u001b[32m   2784\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m missing_names \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m unexpected_names:\n\u001b[32m   2785\u001b[39m     message += \u001b[33m\"\u001b[39m\u001b[33mFeature names must be in the same order as they were in fit.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m2787\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(message)\n",
      "\u001b[31mValueError\u001b[39m: The feature names should match those that were passed during fit.\nFeature names unseen at fit time:\n- APP_CAR\n- APP_COMP_TYPE\n- APP_DRIVING_LICENSE\n- APP_EDUCATION\n- APP_EMP_TYPE\n- ...\n"
     ]
    }
   ],
   "source": [
    "from preprocessing import Preprocessor\n",
    "\n",
    "\n",
    "def predict(model):\n",
    "    preprocessor = Preprocessor()\n",
    "    df_test = preprocessor.load_data('data/bank_data_test.csv')\n",
    "    df_test = preprocessor.transform_with_encoders(df_test, encoders)\n",
    "\n",
    "\n",
    "    df_test = preprocessor.normalize_categorical_columns(df_test)\n",
    "    df_test_encoded = pd.DataFrame(scaler.transform(df_test), columns=df_test.columns)\n",
    "    df = preprocessor.fill_missing_with_mean(df_test_encoded, imputation_values)\n",
    "    \n",
    "    predictions = model.predict(df)\n",
    "    return predictions\n",
    "\n",
    "predictions = predict(dummy_clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61760f8a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
