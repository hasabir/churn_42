{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b322154b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "def load_data(file_path):\n",
    "    \"\"\"Load data from a CSV file into a pandas DataFrame.\"\"\"\n",
    "    return pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d213be15",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_data('data/bank_data_train.csv')\n",
    "missing_pct = df['TARGET'].isnull().sum() / len(df) * 100\n",
    "\n",
    "print(df.shape)\n",
    "print(f\"Percentage of missing TARGET values: {missing_pct:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aae37e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def explore_data(df):\n",
    "    \"\"\"Display basic information about the DataFrame and drop 'ID' if present. Returns the (possibly) modified DataFrame.\"\"\"\n",
    "    print(\"DataFrame Info:\")\n",
    "    df.info()\n",
    "    \n",
    "    # drop unnecessary column if it exists\n",
    "    if 'ID' in df.columns:\n",
    "        df = df.drop(columns=['ID'])\n",
    "        print(\"\\nDropped column: 'ID'\")\n",
    "    else:\n",
    "        print(\"\\nColumn 'ID' not found; skipping drop.\")\n",
    "    \n",
    "    print(\"\\nDataFrame Description (numeric):\")\n",
    "    display(df.describe())\n",
    "    \n",
    "    print(\"\\nDataFrame Description (object):\")\n",
    "    display(df.describe(include=['object']))\n",
    "    \n",
    "    # optionally show missing values\n",
    "    print(\"\\nMissing Values:\")\n",
    "    df_info = pd.DataFrame(df.isnull().sum(), columns=['Missing Values'])\n",
    "    df_info['Percentage'] = (df_info['Missing Values'] / len(df)) * 100\n",
    "    \n",
    "    #siwich column into rows for better display\n",
    "\n",
    "    df_info = df_info.transpose()\n",
    "    display(df_info)\n",
    "    # display(df.isnull().sum())\n",
    "    \n",
    "    # check the distribution of target column\n",
    "    print(df['TARGET'].value_counts())\n",
    "    return df\n",
    "\n",
    "df = explore_data(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d43d644c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e5bc580",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle data types\n",
    "\n",
    "# def convert_data_types(df):\n",
    "#     # get columns with object data type\n",
    "#     obj_cols = df.select_dtypes(include=['object']).columns.tolist()\n",
    "#     print(f\"Object columns to convert: {obj_cols}\")\n",
    "\n",
    "    \n",
    "#     # Apply one-hot encoding to categorical columns\n",
    "#     clean_df = pd.get_dummies(df,\n",
    "#                         columns=obj_cols,\n",
    "#                         drop_first=True).astype(float)\n",
    "#     print(\"Converted object columns to numerical using one-hot encoding.\")\n",
    "#     return clean_df\n",
    "\n",
    "\n",
    "\n",
    "# df = convert_data_types(df.copy())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e278a1b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "existing_values = set()\n",
    "for val in df['APP_MARITAL_STATUS']:\n",
    "    if pd.notnull(val) or val not in existing_values:\n",
    "        existing_values.add(val)\n",
    "existing_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f2efd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For categorical columns, use Chi-Square test or Cramér's V\n",
    "from scipy.stats import chi2_contingency\n",
    "import numpy as np\n",
    "\n",
    "def cramers_v(x, y):\n",
    "    \"\"\"Measure association between categorical x and binary y (TARGET)\"\"\"\n",
    "    confusion_matrix = pd.crosstab(x, y)\n",
    "    chi2 = chi2_contingency(confusion_matrix)[0]\n",
    "    n = confusion_matrix.sum().sum()\n",
    "    phi2 = chi2 / n\n",
    "    r, k = confusion_matrix.shape\n",
    "    return np.sqrt(phi2 / min(k-1, r-1))\n",
    "\n",
    "# Check APP_MARITAL_STATUS\n",
    "# Remove nulls first\n",
    "df_temp = df[['APP_MARITAL_STATUS', 'TARGET']].dropna()\n",
    "correlation = cramers_v(df_temp['APP_MARITAL_STATUS'], df_temp['TARGET'])\n",
    "print(f\"Cramér's V: {correlation:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ca3b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For columns with >70% missing\n",
    "# drop corr bitween -0.05 to 0.05\n",
    "high_missing = df.columns[df.isnull().sum() / len(df) > 0.70]\n",
    "for col in high_missing:\n",
    "    if col == 'TARGET':\n",
    "        continue\n",
    "    # coerce to numeric where possible, compute correlation with TARGET\n",
    "    \n",
    "    if df[col].dtype == 'object':\n",
    "        # categorical column\n",
    "        corr = cramers_v(df[col].dropna(), df.loc[df[col].notnull(), 'TARGET'])\n",
    "    else:\n",
    "        # numerical column\n",
    "        series_num = pd.to_numeric(df[col], errors='coerce')\n",
    "        corr = series_num.corr(df['TARGET'])\n",
    "    \n",
    "    print(type(corr))\n",
    "    print(type(corr))\n",
    "    \n",
    "    # series_num = pd.to_numeric(df[col], errors='coerce')\n",
    "    \n",
    "    \n",
    "    \n",
    "    # corr = cramers_v(df[col].dropna(), df.loc[df[col].notnull(), 'TARGET'])\n",
    "    missing_pct = df[col].isnull().sum() / len(df) * 100\n",
    "    corr_str = f\"{corr:.3f}\" if pd.notnull(corr) else \"N/A\"\n",
    "    print(f\"{col}: Missing={missing_pct:.1f}%, Correlation={corr_str}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "567435c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8862cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For categorical columns, use Chi-Square test or Cramér's V\n",
    "from scipy.stats import chi2_contingency\n",
    "import numpy as np\n",
    "\n",
    "def cramers_v(x, y):\n",
    "    \"\"\"Measure association between categorical x and binary y (TARGET)\"\"\"\n",
    "    confusion_matrix = pd.crosstab(x, y)\n",
    "    chi2 = chi2_contingency(confusion_matrix)[0]\n",
    "    n = confusion_matrix.sum().sum()\n",
    "    phi2 = chi2 / n\n",
    "    r, k = confusion_matrix.shape\n",
    "    return np.sqrt(phi2 / min(k-1, r-1))\n",
    "\n",
    "# Check APP_MARITAL_STATUS\n",
    "# Remove nulls first\n",
    "df_temp = df[['APP_MARITAL_STATUS', 'TARGET']].dropna()\n",
    "correlation = cramers_v(df_temp['APP_MARITAL_STATUS'], df_temp['TARGET'])\n",
    "print(f\"Cramér's V: {correlation:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb51584",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd18e129",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_data('data/bank_data_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a4462fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def normalize_categorical_columns(df):\n",
    "    \"\"\"\n",
    "    Normalize categorical columns by converting to lowercase/uppercase\n",
    "    and stripping whitespace\n",
    "    \"\"\"\n",
    "    cat_cols = df.select_dtypes(include=['object']).columns.tolist()\n",
    "    \n",
    "    for col in cat_cols:\n",
    "        # Convert to string, lowercase, and strip whitespace\n",
    "        df[col] = df[col].astype(str).str.upper()\n",
    "        df[col] = df[col].str.strip()\n",
    "        df[col] = df[col].replace('NAN', np.nan)  # Replace 'NA' strings with NaN\n",
    "        df[col] = df[col].replace('', np.nan)  # Replace empty strings with NaN\n",
    "    \n",
    "    return df\n",
    "df = normalize_categorical_columns(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eacbde4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df['APP_MARITAL_STATUS'].value_counts())\n",
    "# count null values\n",
    "print(\"null values\",df['APP_MARITAL_STATUS'].isnull().sum())\n",
    "print(\"*******************************\")\n",
    "existing_values = set()\n",
    "for val in df['APP_MARITAL_STATUS']:\n",
    "    if pd.notnull(val) or val not in existing_values:\n",
    "        existing_values.add(val)\n",
    "existing_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75167e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "existing_values = []\n",
    "for val in df['APP_MARITAL_STATUS']:\n",
    "    if pd.notnull(val) or val not in existing_values:\n",
    "        existing_values.add(val)\n",
    "existing_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20e79dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle data types\n",
    "\n",
    "def convert_data_types(df):\n",
    "    print(\"Original shape:\", df.shape)\n",
    "    \n",
    "    # Get object columns\n",
    "    obj_cols = df.select_dtypes(include=['object']).columns.tolist()\n",
    "    print(f\"Object columns: {obj_cols}\")\n",
    "    \n",
    "    # First, try to convert object columns to numeric where possible\n",
    "    for col in obj_cols:\n",
    "        try:\n",
    "            # Try to convert to numeric\n",
    "            converted = pd.to_numeric(df[col], errors='coerce')\n",
    "            # If at least 90% conversion success, use numeric\n",
    "            if converted.notna().mean() > 0.9:\n",
    "                df[col] = converted\n",
    "                print(f\"Converted {col} to numeric\")\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    # Update object columns list after numeric conversion\n",
    "    obj_cols = df.select_dtypes(include=['object']).columns.tolist()\n",
    "    \n",
    "    # Identify which columns to one-hot encode\n",
    "    # Typically: low cardinality categorical columns\n",
    "    cat_cols_to_encode = []\n",
    "    for col in obj_cols:\n",
    "        # Only one-hot encode if low cardinality (e.g., less than 10 unique values)\n",
    "        if df[col].nunique() < 10:\n",
    "            cat_cols_to_encode.append(col)\n",
    "        else:\n",
    "            print(f\"Skipping one-hot for {col}: {df[col].nunique()} unique values\")\n",
    "    \n",
    "    print(f\"Categorical columns to one-hot encode: {cat_cols_to_encode}\")\n",
    "    \n",
    "    # Apply one-hot encoding only to selected categorical columns\n",
    "    df = pd.get_dummies(df, columns=cat_cols_to_encode, drop_first=True)\n",
    "    \n",
    "    # For high-cardinality categorical columns, consider other encoding strategies\n",
    "    # or handle them separately\n",
    "    \n",
    "    print(\"Final shape:\", df.shape)\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "df_cleaned = convert_data_types(df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02322f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for col in df_cleaned.columns:\n",
    "    if col.startswith('APP_MARITAL_STATUS'):\n",
    "        print(col)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd1df4ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check existence\n",
    "print('CLNT_TRUST_RELATION' in df.columns)\n",
    "\n",
    "# list generated dummy columns\n",
    "[d for d in df.columns if d.startswith('CLNT_TRUST_RELATION')][:20]\n",
    "\n",
    "# print the dummy columns (or subset)\n",
    "df.filter(regex='^CLNT_TRUST_RELATION').head()\n",
    "\n",
    "display(df.head())\n",
    "obj_cols = df.select_dtypes(include=['object']).columns.tolist()\n",
    "print(f\"Object columns to convert: {obj_cols}\")\n",
    "\n",
    "\n",
    "# print(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6577477",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in df:\n",
    "    column_type = df[col].dtype\n",
    "    if column_type != 'int64' and column_type != 'float64':\n",
    "        print(f\"{col}: {column_type}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dddd8f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0eedbda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def clean_and_encode(df):\n",
    "    \n",
    "    # print(\"Initial data types:\")\n",
    "    # for col in df.columns:\n",
    "    #     if df[col].dtype != 'int64' and df[col].dtype != 'float64':\n",
    "    #         print(f\"  {col}: {df[col].dtype}\")\n",
    "    \n",
    "    # Separate columns by type\n",
    "object_cols = df.select_dtypes(include=['object']).columns.tolist()\n",
    "bool_cols = df.select_dtypes(include=['bool']).columns.tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5670ec0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(f\"\\nObject columns to encode ({len(object_cols)}): {object_cols}\")\n",
    "print(f\"Boolean columns to convert to int ({len(bool_cols)}): {bool_cols[:5]}...\")  # Show first 5\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb535768",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print only the rows that are not null\n",
    "for row in df['APP_MARITAL_STATUS']:\n",
    "    if pd.notnull(row):\n",
    "        print(row)\n",
    "# print(df['APP_TRAVEL_PASS'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb485d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode object columns\n",
    "if object_cols:\n",
    "    df = pd.get_dummies(df, columns=object_cols, drop_first=True, dtype=int)\n",
    "\n",
    "# Convert boolean columns to int\n",
    "if bool_cols:\n",
    "    df[bool_cols] = df[bool_cols].astype(int)\n",
    "\n",
    "# Verify all columns are now numeric\n",
    "print(\"\\nFinal data types:\")\n",
    "numeric_cols = df.select_dtypes(include=['int64', 'int32', 'int', 'float64', 'float32', 'float']).columns\n",
    "non_numeric = [col for col in df.columns if col not in numeric_cols]\n",
    "\n",
    "if non_numeric:\n",
    "    print(f\"Warning: {len(non_numeric)} non-numeric columns remain: {non_numeric}\")\n",
    "else:\n",
    "    print(\"All columns are now numeric!\")\n",
    "\n",
    "print(f\"Final shape: {df.shape}\")\n",
    "#     return df\n",
    "\n",
    "# # Apply the cleaning\n",
    "# df = clean_and_encode(df.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee79638b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_data('data/bank_data_train.csv')\n",
    "\n",
    "def convert_data_types_fixed(df, target_col='TARGET'):\n",
    "    df = df.copy()\n",
    "    # Preserve ID and target if present\n",
    "    has_id = 'ID' in df.columns\n",
    "    ids = df['ID'].copy() if has_id else None\n",
    "    has_target = target_col in df.columns\n",
    "    y = df[target_col].copy() if has_target else None\n",
    "\n",
    "    # Drop ID and target for processing\n",
    "    drop_cols = []\n",
    "    if has_id:\n",
    "        drop_cols.append('ID')\n",
    "    if has_target:\n",
    "        drop_cols.append(target_col)\n",
    "    if drop_cols:\n",
    "        df = df.drop(columns=drop_cols)\n",
    "\n",
    "    # Try numeric conversion for object columns\n",
    "    obj_cols = df.select_dtypes(include=['object']).columns.tolist()\n",
    "    for col in obj_cols:\n",
    "        conv = pd.to_numeric(df[col], errors='coerce')\n",
    "        if conv.notna().mean() > 0.9:\n",
    "            df[col] = conv\n",
    "\n",
    "    # Remaining categorical (object) columns\n",
    "    cat_cols = df.select_dtypes(include=['object']).columns.tolist()\n",
    "    if cat_cols:\n",
    "        low_card = [c for c in cat_cols if df[c].nunique() < 10]\n",
    "        high_card = [c for c in cat_cols if df[c].nunique() >= 10]\n",
    "\n",
    "        # One-hot encode low cardinality\n",
    "        if low_card:\n",
    "            df = pd.get_dummies(df, columns=low_card, drop_first=True, dtype=int)\n",
    "\n",
    "        # Drop high cardinality columns (or handle differently)\n",
    "        if high_card:\n",
    "            print(f\"Dropping high cardinality columns: {high_card}\")\n",
    "            df = df.drop(columns=high_card)\n",
    "\n",
    "    # Re-add ID and target if they existed\n",
    "    if has_id:\n",
    "        df.insert(0, 'ID', ids)\n",
    "    if has_target:\n",
    "        df[target_col] = y\n",
    "\n",
    "    return df\n",
    "\n",
    "# Apply to single dataframe\n",
    "df = convert_data_types_fixed(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea3e2ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.countplot(x='TARGET', data=df)\n",
    "plt.title('Distribution of Churn Target Variable')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98c655f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import category_encoders as ce\n",
    "import pandas as pd\n",
    "\n",
    "def encode_categorical_data(df, target):\n",
    "    \"\"\"\n",
    "    Convert object columns to appropriate numeric representations.\n",
    "    Note: For production, split train/test BEFORE calling this function\n",
    "    and fit encoders only on training data.\n",
    "    \"\"\"\n",
    "    print(\"Original shape:\", df.shape)\n",
    "    \n",
    "    # Get object columns\n",
    "    obj_cols = df.select_dtypes(include=['object']).columns.tolist()\n",
    "    print(f\"Object columns: {obj_cols}\")\n",
    "    \n",
    "    # Try to convert object columns to numeric where possible\n",
    "    for col in obj_cols:\n",
    "        try:\n",
    "            converted = pd.to_numeric(df[col], errors='coerce')\n",
    "            if converted.notna().mean() > 0.9:\n",
    "                df[col] = converted\n",
    "                print(f\"Converted {col} to numeric\")\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    # Update object columns list after numeric conversion\n",
    "    obj_cols = df.select_dtypes(include=['object']).columns.tolist()\n",
    "    \n",
    "    # Separate columns by cardinality\n",
    "    cat_cols_to_encode = []  # Low cardinality (one-hot)\n",
    "    high_card_cols = []      # High cardinality (target encode)\n",
    "    cols_to_drop = []        # All null columns\n",
    "    \n",
    "    for col in obj_cols:\n",
    "        # if col == 'TARGET':\n",
    "        #     continue\n",
    "            \n",
    "        # Skip columns with all nulls\n",
    "        if df[col].isnull().all():\n",
    "            print(f\"Skipping {col} (all null)\")\n",
    "            cols_to_drop.append(col)\n",
    "            continue\n",
    "        \n",
    "        unique_count = df[col].nunique(dropna=True)\n",
    "        \n",
    "        # Categorize by cardinality\n",
    "        if unique_count < 10:\n",
    "            cat_cols_to_encode.append(col)\n",
    "            print(f\"Low cardinality {col}: {unique_count} unique values → One-hot encode\")\n",
    "        else:\n",
    "            high_card_cols.append(col)\n",
    "            print(f\"High cardinality {col}: {unique_count} unique values → Target encode\")\n",
    "    \n",
    "    # Drop all-null columns\n",
    "    if cols_to_drop:\n",
    "        df = df.drop(columns=cols_to_drop)\n",
    "        print(f\"Dropped all-null columns: {cols_to_drop}\")\n",
    "    \n",
    "    # Target encode high-cardinality columns\n",
    "    if high_card_cols:\n",
    "        print(f\"\\nApplying Target Encoding to: {high_card_cols}\")\n",
    "        \n",
    "        for col in high_card_cols:\n",
    "            try:\n",
    "                # Create mask for valid (non-null) values\n",
    "                valid_mask = df[col].notna() & target['TARGET'].notna()\n",
    "                \n",
    "                # Initialize encoder\n",
    "                encoder = ce.TargetEncoder(cols=[col], smoothing=1.0)\n",
    "                \n",
    "                # Fit and transform on valid data\n",
    "                df.loc[valid_mask, f\"{col}_ENCODED\"] = encoder.fit_transform(\n",
    "                    df.loc[valid_mask, [col]], \n",
    "                    target.loc[valid_mask, 'TARGET']\n",
    "                )[col]\n",
    "                \n",
    "                # For null values, use global mean of target\n",
    "                global_mean = target['TARGET'].mean()\n",
    "                df[f\"{col}_ENCODED\"].fillna(global_mean, inplace=True)\n",
    "                \n",
    "                print(f\"✓ Target encoded: {col}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"✗ Target encoding failed for {col}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        # Drop original high-cardinality columns after encoding\n",
    "        df = df.drop(columns=high_card_cols)\n",
    "    \n",
    "    # One-hot encode low-cardinality columns\n",
    "    if cat_cols_to_encode:\n",
    "        print(f\"\\nOne-hot encoding: {cat_cols_to_encode}\")\n",
    "        df = pd.get_dummies(df, columns=cat_cols_to_encode, drop_first=True)\n",
    "    \n",
    "    print(f\"\\nFinal shape: {df.shape}\")\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7759575c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test conversion\n",
    "ds = encode_categorical_data(df, df)\n",
    "\n",
    "\n",
    "\n",
    "for col in ds:\n",
    "    column_type = ds[col].dtype\n",
    "    if column_type != 'int64' and column_type != 'float64':\n",
    "        print(f\"{col}: {column_type}\")\n",
    "\n",
    "\n",
    "# Count non-null occurrences of each job position\n",
    "existing_values = {}\n",
    "for val in ds['CLNT_JOB_POSITION_ENCODED']:\n",
    "    if pd.isnull(val):\n",
    "        continue\n",
    "    existing_values[val] = existing_values.get(val, 0) + 1\n",
    "corr = cramers_v(ds['CLNT_JOB_POSITION_ENCODED'].dropna(), ds.loc[ds['CLNT_JOB_POSITION_ENCODED'].notnull(), 'TARGET'])\n",
    "missing_pct = ds['CLNT_JOB_POSITION_ENCODED'].isnull().sum() / len(ds) * 100\n",
    "print(f\"CLNT_JOB_POSITION: Missing={missing_pct:.1f}%, Correlation={corr:.3f}\")\n",
    "print(f\"existing_values length: {len(existing_values)}\")\n",
    "existing_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "816689d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import category_encoders as ce\n",
    "import pandas as pd\n",
    "\n",
    "def fit_encoders(X_train, y_train):\n",
    "    \"\"\"\n",
    "    Fit encoders on training data and return fitted encoders.\n",
    "    \"\"\"\n",
    "    # Get object columns\n",
    "    obj_cols = X_train.select_dtypes(include=['object']).columns.tolist()\n",
    "    \n",
    "    # Try numeric conversion\n",
    "    numeric_conversions = {}\n",
    "    for col in obj_cols:\n",
    "        converted = pd.to_numeric(X_train[col], errors='coerce')\n",
    "        if converted.notna().mean() > 0.9:\n",
    "            numeric_conversions[col] = True\n",
    "    print(\"numeric conversion\", numeric_conversions)\n",
    "    # Update object columns\n",
    "    obj_cols = [col for col in obj_cols if col not in numeric_conversions]\n",
    "    \n",
    "    # Categorize by cardinality\n",
    "    low_card_cols = []\n",
    "    high_card_cols = []\n",
    "    \n",
    "    for col in obj_cols:\n",
    "        if X_train[col].isnull().all():\n",
    "            continue\n",
    "        \n",
    "        unique_count = X_train[col].nunique(dropna=True)\n",
    "        \n",
    "        if unique_count < 10:\n",
    "            low_card_cols.append(col)\n",
    "        else:\n",
    "            high_card_cols.append(col)\n",
    "    \n",
    "    # Fit target encoder on training data\n",
    "    target_encoder = None\n",
    "    if high_card_cols:\n",
    "        target_encoder = ce.TargetEncoder(cols=high_card_cols, smoothing=1.0)\n",
    "        valid_mask = X_train[high_card_cols].notna().all(axis=1) & y_train['TARGET'].notna()\n",
    "        target_encoder.fit(X_train.loc[valid_mask, high_card_cols], y_train.loc[valid_mask, 'TARGET'])\n",
    "    \n",
    "    return {\n",
    "        'numeric_conversions': numeric_conversions,\n",
    "        'low_card_cols': low_card_cols,\n",
    "        'high_card_cols': high_card_cols,\n",
    "        'target_encoder': target_encoder,\n",
    "        'global_mean': y_train['TARGET'].mean()\n",
    "    }\n",
    "\n",
    "\n",
    "def transform_with_encoders(X, encoders_dict):\n",
    "    \"\"\"\n",
    "    Transform data using pre-fitted encoders.\n",
    "    \"\"\"\n",
    "    X = X.copy()\n",
    "    \n",
    "    # Apply numeric conversions\n",
    "    for col in encoders_dict['numeric_conversions']:\n",
    "        X[col] = pd.to_numeric(X[col], errors='coerce')\n",
    "    \n",
    "    # Drop all-null columns\n",
    "    obj_cols = X.select_dtypes(include=['object']).columns\n",
    "    for col in obj_cols:\n",
    "        if X[col].isnull().all():\n",
    "            X = X.drop(columns=[col])\n",
    "    \n",
    "    # Target encode high-cardinality columns\n",
    "    if encoders_dict['high_card_cols'] and encoders_dict['target_encoder']:\n",
    "        high_card_cols = encoders_dict['high_card_cols']\n",
    "        print(f\"\\n\\nTarget encoding: {high_card_cols}\\n\\n\")\n",
    "        for col in high_card_cols:\n",
    "            if col in X.columns:\n",
    "                # Transform using fitted encoder\n",
    "                print(f\"Encoding column: {col}\")\n",
    "                print()\n",
    "                try:\n",
    "                    # Transform using the encoder fitted on all high-cardinality cols,\n",
    "                    # then take the encoded series for the current column\n",
    "                    encoded = encoders_dict['target_encoder'].transform(X[encoders_dict['high_card_cols']])[col]\n",
    "                    X[col] = encoded\n",
    "                    # Fill unseen / missing encodings with global mean\n",
    "                    X[col].fillna(encoders_dict['global_mean'], inplace=True)\n",
    "                except Exception:\n",
    "                    # If transform fails, fall back to global mean\n",
    "                    X[col] = encoders_dict['global_mean']\n",
    "        #         X = X.drop(columns=[col])\n",
    "                \n",
    "                \n",
    "                \n",
    "        #         X[f\"{col}\"] = encoded[col]\n",
    "                \n",
    "        #         # Fill nulls with global mean\n",
    "        #         X[f\"{col}\"].fillna(encoders_dict['global_mean'], inplace=True)\n",
    "        \n",
    "        # Drop original columns\n",
    "        # X = X.drop(columns=[col for col in high_card_cols if col in X.columns])\n",
    "    \n",
    "    # One-hot encode low-cardinality columns\n",
    "    if encoders_dict['low_card_cols']:\n",
    "        print(f\"\\nOne-hot encoding: {encoders_dict['low_card_cols']}\")\n",
    "\n",
    "        X = pd.get_dummies(X, columns=encoders_dict['low_card_cols'], drop_first=True)\n",
    "    \n",
    "    return X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a57babf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data train test\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "y = pd.DataFrame()\n",
    "y['TARGET'] = df['TARGET']\n",
    "X = df.drop(columns=['TARGET'])\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "                            X,y,\n",
    "                            test_size=0.2,\n",
    "                            random_state=42,\n",
    "                            stratify=y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d53bf6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Split data\n",
    "y = df[['TARGET']]\n",
    "X = df.drop(columns=['TARGET'])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Train shape: {X_train.shape}, Test shape: {X_test.shape}\")\n",
    "\n",
    "# Fit encoders on TRAINING data only\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Fitting encoders on training data...\")\n",
    "print(\"=\"*80)\n",
    "encoders = fit_encoders(X_train, y_train)\n",
    "for key, value in encoders.items():\n",
    "        print(f\"{key}: {value}\")\n",
    "# # Transform both train and test using the SAME encoders\n",
    "print(\"\\nTransforming training data...\")\n",
    "X_train_encoded = transform_with_encoders(X_train, encoders)\n",
    "\n",
    "print(\"\\nTransforming test data...\")\n",
    "X_test_encoded = transform_with_encoders(X_test, encoders)\n",
    "\n",
    "\n",
    "print(f\"\\nAfter encoding - Train: {X_train_encoded.shape}, Test: {X_test_encoded.shape}\")\n",
    "\n",
    "# Just to make sure not needed in our case -------------------------------------\n",
    "# Align columns (ensure train and test have same columns)\n",
    "print(\"\\nAligning train/test columns...\")\n",
    "train_cols = set(X_train_encoded.columns)\n",
    "test_cols = set(X_test_encoded.columns)\n",
    "\n",
    "# Add missing columns to test (fill with 0)\n",
    "for col in train_cols - test_cols:\n",
    "    X_test_encoded[col] = 0\n",
    "    print(f\"Added missing column to test: {col}\")\n",
    "\n",
    "# Remove extra columns from test\n",
    "for col in test_cols - train_cols:\n",
    "    X_test_encoded = X_test_encoded.drop(columns=[col])\n",
    "    print(f\"Removed extra column from test: {col}\")\n",
    "#-------------------------------------------------------------------------------------\n",
    "\n",
    "# # Reorder test columns to match train\n",
    "# X_test_encoded = X_test_encoded[X_train_encoded.columns]\n",
    "\n",
    "# print(f\"\\nAfter alignment - Train: {X_train_encoded.shape}, Test: {X_test_encoded.shape}\")\n",
    "\n",
    "# # Impute missing values in BOTH train and test\n",
    "# print(\"\\n\" + \"=\"*80)\n",
    "# print(\"Imputing missing values...\")\n",
    "# print(\"=\"*80)\n",
    "\n",
    "# # Calculate means from TRAINING data only\n",
    "# imputation_values = X_train_encoded.mean()\n",
    "\n",
    "# # Apply to both train and test\n",
    "# for col in X_train_encoded.columns:\n",
    "#     if X_train_encoded[col].isnull().any():\n",
    "#         X_train_encoded[col].fillna(imputation_values[col], inplace=True)\n",
    "#         print(f\"Imputed train {col}: {imputation_values[col]:.4f}\")\n",
    "    \n",
    "#     if X_test_encoded[col].isnull().any():\n",
    "#         X_test_encoded[col].fillna(imputation_values[col], inplace=True)\n",
    "#         print(f\"Imputed test {col}: {imputation_values[col]:.4f}\")\n",
    "\n",
    "# # Verify no nulls remain\n",
    "# print(f\"\\nTrain nulls: {X_train_encoded.isnull().sum().sum()}\")\n",
    "# print(f\"Test nulls: {X_test_encoded.isnull().sum().sum()}\")\n",
    "\n",
    "# # Apply SMOTE to training data only\n",
    "# print(\"\\n\" + \"=\"*80)\n",
    "# print(\"Applying SMOTE to training data...\")\n",
    "# print(\"=\"*80)\n",
    "# print(f\"Before SMOTE: {y_train['TARGET'].value_counts().to_dict()}\")\n",
    "\n",
    "# smote = SMOTE(sampling_strategy='minority', random_state=42)\n",
    "# X_train_balanced, y_train_balanced = smote.fit_resample(X_train_encoded, y_train)\n",
    "\n",
    "# print(f\"After SMOTE: {y_train_balanced['TARGET'].value_counts().to_dict()}\")\n",
    "# print(f\"Train shape: {X_train_balanced.shape}\")\n",
    "\n",
    "# print(\"\\n\" + \"=\"*80)\n",
    "# print(\"Data preprocessing complete!\")\n",
    "# print(\"=\"*80)\n",
    "# print(f\"Final shapes:\")\n",
    "# print(f\"  X_train_balanced: {X_train_balanced.shape}\")\n",
    "# print(f\"  y_train_balanced: {y_train_balanced.shape}\")\n",
    "# print(f\"  X_test_encoded: {X_test_encoded.shape}\")\n",
    "# print(f\"  y_test: {y_test.shape}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
